{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Necessary packages and paths are loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup Python path to include the project root\n",
    "project_dir = Path.cwd().parent\n",
    "if str(project_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import the agent and state management class\n",
    "from ProductDesigner.agents.safe_product_interviewer import SafeProductInterviewerAgent, logging\n",
    "from ProductDesigner.graph_state import GraphState\n",
    "\n",
    "# Imports neccessary to run the cells below\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "print('Setup complete. Necessary packages and paths are loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Agent Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:39:55,051 - INFO - LLMService initialized with model: gpt-4o\n",
      "2025-08-07 23:39:55,372 - INFO - Web search is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What is the primary purpose of this application or system?',\n",
       " 'Who are the main users or target audience of this application?',\n",
       " 'What are the 3-5 core features needed for the MVP (Minimum Viable Product)?',\n",
       " 'What specific problem does this solution solve for your users?',\n",
       " 'Are there any existing solutions to this problem? How is yours different?',\n",
       " 'What technologies or tech stack do you have in mind? Or would you like recommendations?',\n",
       " 'What are the main user flows or journeys through the application?',\n",
       " 'Are there any specific UI/UX requirements or preferences?',\n",
       " 'What are your plans for data storage and management?',\n",
       " 'Are there any specific security requirements or concerns?',\n",
       " 'What is your timeline for development of the MVP?',\n",
       " 'Are there any third-party integrations needed?',\n",
       " 'What metrics would define success for this product?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the initial product idea and create a mock state\n",
    "initial_idea = \"An AI-powered mutual fund recommendation system for indian MFs - fetching data from public or 3rd part API, produce vizualizations, track investments, suggest switches, analyze performanc eo fmultuaal funds, determine best MFs for a given risk type, optimal allocation\"\n",
    "mock_state = {\"initial_idea\": initial_idea, \"qna_history\": {}}\n",
    "\n",
    "# Initialize the interviewer agent\n",
    "interviewer = SafeProductInterviewerAgent()\n",
    "print(\"Agent initialized successfully.\")\n",
    "\n",
    "questions = interviewer._get_planning_questions()\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 1: Get the Answers of the Questions I have prepared previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 01:40:50,479 - INFO - Executing LLM chain with params: dict_keys(['questions', 'initial_idea', 'format_instructions'])\n",
      "2025-07-31 01:41:10,330 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-31 01:41:10,347 - INFO - LLM chain executed successfully.\n"
     ]
    }
   ],
   "source": [
    "class QA(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class QALIST(BaseModel):\n",
    "    questions: List[QA]\n",
    "\n",
    "    def str(self):\n",
    "        return '\\n'.join([f\"Question: {qa.question}\\nAnswer: {qa.answer}\" for qa in self.questions])\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=QALIST)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "questions = interviewer._get_planning_questions()\n",
    "str_questions = '\\n'.join(questions)\n",
    "template = (\n",
    "            \"You are an expert product manager. Based on the initial idea and any relevant search results, \"\n",
    "            \"provide a comprehensive and insightful answer to the following questions. Each answer will be displayed to the client for helping them decide their answer. So make each answer as informative as possible with recommendations, options, industry standards, examples of existing products with simiiar focus etc.\\n\\n\"\n",
    "            \"Initial Idea: {initial_idea}\\n\"\n",
    "            \"Questions: \\n {questions}\\n\"\n",
    "            \"Include reasoning, multiple options where applicable, and industry best practices.\\n\"\n",
    "            \"Answer Format Instructions: \"\n",
    "            \"\\n{format_instructions}\"\n",
    "        )\n",
    "\n",
    "params = {\"questions\": str_questions, \n",
    "            \"initial_idea\": initial_idea,\n",
    "            \"format_instructions\": format_instructions}\n",
    "initial_qa = interviewer._run_llm_step(dict(), template, params, \"Could not generate an initial answer due to an error.\", output_parser=parser, model_override=\"gpt-4.1\", temparature_override=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 2: Create Web search from the answers for enriching the Answers Further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 01:52:18,155 - INFO - Executing LLM chain with params: dict_keys(['initial_idea', 'format_instructions', 'initial_qa'])\n",
      "2025-07-31 01:52:23,207 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-31 01:52:23,220 - INFO - LLM chain executed successfully.\n"
     ]
    }
   ],
   "source": [
    "class Websearch(BaseModel):\n",
    "    search_queries: List[str]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Websearch)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "template = (\n",
    "            \"You are an expert product manager. Based on the initial idea and the initial QA with the client, generate a list of search queries to perform web search, to enrich the answers\"\n",
    "            \"Each answer will be displayed to the client for helping them decide their answer. So make the queries to extract relevant and timely information. Aim to enrich the recommendations, options, industry standards, examples of existing products with simiiar focus etc.\\n\\n\"\n",
    "            \"Initial Idea: {initial_idea}\\n\"\n",
    "            \"QA: \\n {initial_qa}\\n\"\n",
    "            \"Include reasoning, multiple options where applicable, and industry best practices.\\n\"\n",
    "            \"Answer Format Instructions: \"\n",
    "            \"\\n{format_instructions}\"\n",
    "        )\n",
    "\n",
    "params = {\"initial_idea\": initial_idea,\n",
    "            \"format_instructions\": format_instructions,\n",
    "            \"initial_qa\": initial_qa.str()}\n",
    "\n",
    "websearch = interviewer._run_llm_step(dict(), template, params, \"Could not generate the list of web search queries.\", output_parser=parser, model_override=\"gpt-4.1\", temparature_override=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 05:13:55,675 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class Agent(BaseModel):\n",
    "    name:str\n",
    "    responsibility:str\n",
    "    parent:str\n",
    "    children:List[str]\n",
    "    input:List[str] = Field(default_factory=list, description=\"list of fields from the state that will act as the input for this agent\")\n",
    "    output:List[str] = Field(default_factory=list, description=\"list of fields from the state that will act as the output for this agent\")\n",
    "\n",
    "class State(BaseModel):\n",
    "    property_names: List[str] = Field(default_factory=list, description=\"list of properties of the langraph state for the agentic network\")\n",
    "    property_descriptions: List[str]\n",
    "    property_defaults: List[str]\n",
    "\n",
    "class Graph(BaseModel):\n",
    "    agents: List[Agent]\n",
    "    state: State\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Graph)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "template = (\n",
    "            \"Given the initial idea of the product, design an Langraph based agentic network to produce documents neccessary for an LLM based coding assistant to execute the product idea autonomously.\"\n",
    "            \"Human intervention should be minimal - every human interaction should have a default which the human can simply accept or provide feedback. \"\n",
    "            \"The documents produced should be in markdown format. \"\n",
    "            \"The agentic network should be able to discover information and perform web search to enrich the answers. Use that information to make decisions about the architecture as well as low level implementaation detials.\"\n",
    "            \"The documentation should meet professional standards but targetted at beginners. \\n\\n\"\n",
    "            \"Initial Idea: {initial_idea}\\n\"\n",
    "            \"Answer Format Instructions: \"\n",
    "            \"\\n{format_instructions}\"\n",
    "        )\n",
    "\n",
    "params = {\"initial_idea\": initial_idea,\n",
    "            \"format_instructions\": format_instructions}\n",
    "\n",
    "#websearch = interviewer._run_llm_step(dict(), template, params, \"Could not generate the list of web search queries.\", output_parser=parser, model_override=\"gpt-5\")\n",
    "# Merge the template and params to create teh full prompt\n",
    "full_prompt = template.format(**params)\n",
    "\n",
    "# Call openai\n",
    "client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "            \n",
    "response = await client.responses.create(\n",
    "    model=\"o3\",\n",
    "    input=full_prompt,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"web_search_preview\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "content = response.output_text\n",
    "# extract json from content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(agents=[Agent(name='OrchestratorAgent', responsibility='Root supervisor that sequences all child-agents, resolves dependencies, applies default human choices when no feedback is received, and merges every markdown output into “final_docs”.', parent='root', children=['ProductManagerAgent', 'ArchitectureAgent', 'DevOpsAgent', 'DocumentationAgent'], input=['idea', 'user_feedback'], output=['final_docs']), Agent(name='ProductManagerAgent', responsibility='Transform the raw idea into a beginner-friendly Product-Requirements-Document (PRD) that covers goals, user stories, MVP scope, personas, and compliance assumptions.  Defaults to current idea if the user supplies no extra info.', parent='OrchestratorAgent', children=['ResearchAgent'], input=['idea', 'user_feedback'], output=['prd']), Agent(name='ResearchAgent', responsibility='Perform live web search to discover Indian mutual-fund data sources (e.g., AMFI daily NAV files, mftool library, BSE-Star/NSE-NMF order APIs, 3rd-party projects like amfi-nav-api) and relevant SEBI/AMFI rules.  Summarise findings in markdown with citations. ([amfiindia.com](https://www.amfiindia.com/investor-corner/knowledge-center/net-asset-value.html?utm_source=chatgpt.com), [github.com](https://github.com/m-amaresh/amfi-nav-api?utm_source=chatgpt.com), [mftool.readthedocs.io](https://mftool.readthedocs.io/?utm_source=chatgpt.com))', parent='ProductManagerAgent', children=[], input=['prd'], output=['research_findings']), Agent(name='ArchitectureAgent', responsibility='Design the end-to-end system architecture (data-ingestion micro-service, PostgreSQL data-warehouse, FastAPI backend, Streamlit/React front-end, recommendation engine, authentication, observability). Produce a component diagram and technology rationale.', parent='OrchestratorAgent', children=['DataIngestionAgent', 'BackendDesignAgent', 'FrontendDesignAgent', 'MLModelAgent'], input=['prd', 'research_findings', 'user_feedback'], output=['architecture_doc']), Agent(name='DataIngestionAgent', responsibility='Specify how to fetch, parse and normalise NAV/metadata from AMFI text files; periodically poll custom APIs when available; store raw + curated tables; handle failures & retries; provide sample Python code (using requests & pandas).', parent='ArchitectureAgent', children=[], input=['architecture_doc', 'research_findings'], output=['ingestion_design']), Agent(name='BackendDesignAgent', responsibility='Define REST/GraphQL endpoints for portfolios, MF screening, risk profiling and switch suggestions; outline database schema (SQL-alchemy models) and service-layer classes; include auth & RBAC defaults.', parent='ArchitectureAgent', children=[], input=['architecture_doc', 'ingestion_design'], output=['backend_design']), Agent(name='FrontendDesignAgent', responsibility='Produce UI/UX wireframes and component spec using Streamlit (default) or React if user overrides; describe visualisations (e.g., line chart of NAV history, heat-map of asset allocation) and state-management strategy.', parent='ArchitectureAgent', children=[], input=['architecture_doc', 'backend_design'], output=['frontend_design']), Agent(name='MLModelAgent', responsibility='Outline portfolio-recommendation logic: risk-score calculation (variance, Sharpe), modern portfolio optimisation, rule-based switch alerts, and explainable AI techniques. Provide skeleton Python notebooks and default hyper-parameters.', parent='ArchitectureAgent', children=[], input=['architecture_doc', 'ingestion_design', 'research_findings'], output=['ml_design']), Agent(name='DevOpsAgent', responsibility='Create CI/CD pipeline spec (GitHub Actions), Dockerfiles, Kubernetes Helm chart defaults, secrets handling, logging/metrics stack and cost estimates for a small Indian cloud deployment.', parent='OrchestratorAgent', children=[], input=['architecture_doc', 'user_feedback'], output=['devops_plan']), Agent(name='DocumentationAgent', responsibility='Compile all individual markdown documents (prd, research_findings, architecture_doc, ingestion_design, backend_design, frontend_design, ml_design, devops_plan) into a polished beginner-oriented handbook and update “final_docs”.', parent='OrchestratorAgent', children=[], input=['prd', 'research_findings', 'architecture_doc', 'ingestion_design', 'backend_design', 'frontend_design', 'ml_design', 'devops_plan'], output=['final_docs'])], state=State(property_names=['idea', 'prd', 'research_findings', 'architecture_doc', 'ingestion_design', 'backend_design', 'frontend_design', 'ml_design', 'devops_plan', 'final_docs', 'user_feedback'], property_descriptions=['Initial product concept provided by user', 'Product Requirements Document in markdown', 'Web-research summary with citations', 'High-level system architecture document', 'Data ingestion & ETL specification', 'Backend service design documentation', 'Frontend/UI design documentation', 'Machine-learning & recommendation design', 'DevOps / deployment plan', 'Concatenated deliverables for the coding assistant', 'Optional human comments or approval'], property_defaults=['An AI-powered mutual fund recommendation system for Indian MFs – fetch data, visualise, track, suggest switches, analyse performance, risk-based selection and allocation.', '', '', '', '', '', '', '', '', '', 'accept']))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = parser._parse_obj(json.loads(content))\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 1: Generating a Knowledgebase                                                                                                                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:40:00,406 - INFO - LLMService initialized with model: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from ProductDesigner.agents.research_subgraph_agent import ResearchSubgraphAgent, ResearchState, _execute_parallel_searches_for_queries\n",
    "\n",
    "agent = ResearchSubgraphAgent()\n",
    "research_state = ResearchState(\n",
    "                initial_idea=initial_idea,\n",
    "                max_depth=5\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:40:07,274 - INFO - Generating initial search queries...\n",
      "2025-08-07 23:40:07,310 - INFO - Executing LLM chain with params: dict_keys(['initial_idea', 'format_instructions'])\n",
      "2025-08-07 23:40:14,881 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:14,965 - INFO - LLM chain executed successfully.\n",
      "2025-08-07 23:40:14,967 - INFO - Generated 30 search queries\n",
      "2025-08-07 23:40:14,968 - INFO - 1. Existing AI-powered mutual fund recommendation platforms in India\n",
      "2025-08-07 23:40:14,968 - INFO - 2. Popular mutual fund analysis tools for Indian investors\n",
      "2025-08-07 23:40:14,971 - INFO - 3. Feature comparison of Indian mutual fund tracking apps\n",
      "2025-08-07 23:40:14,971 - INFO - 4. User journey for mutual fund investment and tracking platforms\n",
      "2025-08-07 23:40:14,971 - INFO - 5. Tech stack used by leading mutual fund recommendation systems\n",
      "2025-08-07 23:40:14,973 - INFO - 6. Open source codebases for mutual fund analysis on Github\n",
      "2025-08-07 23:40:14,973 - INFO - 7. Python packages for financial data analysis and visualization\n",
      "2025-08-07 23:40:14,976 - INFO - 8. APIs providing Indian mutual fund data (e.g., Value Research, Morningstar, AMFI)\n",
      "2025-08-07 23:40:14,978 - INFO - 9. UI/UX examples for investment recommendation dashboards\n",
      "2025-08-07 23:40:14,980 - INFO - 10. Core functional modules in mutual fund recommendation systems\n",
      "2025-08-07 23:40:14,980 - INFO - 11. Gap analysis between existing mutual fund platforms and AI-powered recommendations\n",
      "2025-08-07 23:40:14,982 - INFO - 12. Features commonly excluded from MVPs in investment tracking apps\n",
      "2025-08-07 23:40:14,982 - INFO - 13. Useful but non-essential features for mutual fund recommendation tools\n",
      "2025-08-07 23:40:14,984 - INFO - 14. Data sources for Indian mutual fund NAVs, performance, and holdings\n",
      "2025-08-07 23:40:14,984 - INFO - 15. API documentation for fetching Indian mutual fund data\n",
      "2025-08-07 23:40:14,986 - INFO - 16. Visualization libraries for financial data in Python\n",
      "2025-08-07 23:40:14,986 - INFO - 17. Best practices for risk profiling in investment recommendation engines\n",
      "2025-08-07 23:40:14,989 - INFO - 18. Switch suggestion algorithms for mutual fund portfolios\n",
      "2025-08-07 23:40:14,990 - INFO - 19. Optimal asset allocation algorithms for mutual fund investments\n",
      "2025-08-07 23:40:14,992 - INFO - 20. Security and compliance considerations for investment platforms in India\n",
      "2025-08-07 23:40:14,992 - INFO - 21. User authentication and portfolio privacy in investment tracking apps\n",
      "2025-08-07 23:40:14,994 - INFO - 22. Integration of third-party APIs for real-time mutual fund data\n",
      "2025-08-07 23:40:14,995 - INFO - 23. Examples of personalized investment recommendation engines\n",
      "2025-08-07 23:40:14,995 - INFO - 24. Machine learning models for mutual fund performance prediction\n",
      "2025-08-07 23:40:14,995 - INFO - 25. Open source UI component libraries for financial dashboards\n",
      "2025-08-07 23:40:14,998 - INFO - 26. Comparison of mobile vs web platforms for mutual fund tracking\n",
      "2025-08-07 23:40:14,998 - INFO - 27. User feedback and review analysis for existing mutual fund apps\n",
      "2025-08-07 23:40:15,001 - INFO - 28. Challenges in tracking and analyzing Indian mutual fund investments\n",
      "2025-08-07 23:40:15,003 - INFO - 29. Regulatory requirements for financial advisory platforms in India\n",
      "2025-08-07 23:40:15,004 - INFO - 30. Benchmarks for evaluating mutual fund recommendation accuracy\n",
      "2025-08-07 23:40:15,005 - INFO - Generated 30 queries for depth 1\n"
     ]
    }
   ],
   "source": [
    "research_state = agent._generate_queries_node(research_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:40:15,028 - INFO - ---1 ---\n",
      "2025-08-07 23:40:15,029 - INFO - --- Existing AI-powered mutual fund recommendation platforms in India ---\n",
      "2025-08-07 23:40:15,031 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,032 - INFO - --- Popular mutual fund analysis tools for Indian investors ---\n",
      "2025-08-07 23:40:15,035 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,037 - INFO - --- Feature comparison of Indian mutual fund tracking apps ---\n",
      "2025-08-07 23:40:15,039 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,040 - INFO - --- User journey for mutual fund investment and tracking platforms ---\n",
      "2025-08-07 23:40:15,042 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,043 - INFO - --- Tech stack used by leading mutual fund recommendation systems ---\n",
      "2025-08-07 23:40:15,045 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,046 - INFO - --- Open source codebases for mutual fund analysis on Github ---\n",
      "2025-08-07 23:40:15,046 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,049 - INFO - --- Python packages for financial data analysis and visualization ---\n",
      "2025-08-07 23:40:15,050 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,051 - INFO - --- APIs providing Indian mutual fund data (e.g., Value Research, Morningstar, AMFI) ---\n",
      "2025-08-07 23:40:15,051 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,054 - INFO - --- UI/UX examples for investment recommendation dashboards ---\n",
      "2025-08-07 23:40:15,056 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,056 - INFO - --- Core functional modules in mutual fund recommendation systems ---\n",
      "2025-08-07 23:40:15,058 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,061 - INFO - --- Gap analysis between existing mutual fund platforms and AI-powered recommendations ---\n",
      "2025-08-07 23:40:15,061 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,063 - INFO - --- Features commonly excluded from MVPs in investment tracking apps ---\n",
      "2025-08-07 23:40:15,064 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,066 - INFO - --- Useful but non-essential features for mutual fund recommendation tools ---\n",
      "2025-08-07 23:40:15,067 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,068 - INFO - --- Data sources for Indian mutual fund NAVs, performance, and holdings ---\n",
      "2025-08-07 23:40:15,068 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,070 - INFO - --- API documentation for fetching Indian mutual fund data ---\n",
      "2025-08-07 23:40:15,072 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,074 - INFO - --- Visualization libraries for financial data in Python ---\n",
      "2025-08-07 23:40:15,074 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,074 - INFO - --- Best practices for risk profiling in investment recommendation engines ---\n",
      "2025-08-07 23:40:15,074 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,080 - INFO - --- Switch suggestion algorithms for mutual fund portfolios ---\n",
      "2025-08-07 23:40:15,082 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,084 - INFO - --- Optimal asset allocation algorithms for mutual fund investments ---\n",
      "2025-08-07 23:40:15,084 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,084 - INFO - --- Security and compliance considerations for investment platforms in India ---\n",
      "2025-08-07 23:40:15,087 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,088 - INFO - --- User authentication and portfolio privacy in investment tracking apps ---\n",
      "2025-08-07 23:40:15,088 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,088 - INFO - --- Integration of third-party APIs for real-time mutual fund data ---\n",
      "2025-08-07 23:40:15,088 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,088 - INFO - --- Examples of personalized investment recommendation engines ---\n",
      "2025-08-07 23:40:15,094 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,096 - INFO - --- Machine learning models for mutual fund performance prediction ---\n",
      "2025-08-07 23:40:15,096 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,097 - INFO - --- Open source UI component libraries for financial dashboards ---\n",
      "2025-08-07 23:40:15,097 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,097 - INFO - --- Comparison of mobile vs web platforms for mutual fund tracking ---\n",
      "2025-08-07 23:40:15,101 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,101 - INFO - --- User feedback and review analysis for existing mutual fund apps ---\n",
      "2025-08-07 23:40:15,104 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,105 - INFO - --- Challenges in tracking and analyzing Indian mutual fund investments ---\n",
      "2025-08-07 23:40:15,107 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,109 - INFO - --- Regulatory requirements for financial advisory platforms in India ---\n",
      "2025-08-07 23:40:15,110 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,112 - INFO - --- Benchmarks for evaluating mutual fund recommendation accuracy ---\n",
      "2025-08-07 23:40:15,116 - INFO - --- openai_web_search ---\n",
      "2025-08-07 23:40:15,118 - INFO - --- 2 ---\n",
      "2025-08-07 23:40:15,119 - INFO - Executing 30 search tasks in parallel across 30 queries and 1 protocols\n",
      "2025-08-07 23:40:33,110 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:34,346 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:35,366 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:35,371 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:35,775 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:36,185 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:36,500 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:36,504 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:36,596 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:36,605 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:36,737 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:37,006 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:37,011 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:37,130 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:37,197 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:37,526 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:37,531 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:37,658 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:38,233 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:38,239 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:38,448 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:38,639 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:39,133 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:39,565 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:39,569 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:40,690 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:40,895 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:43,969 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:44,482 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:47,758 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:47,766 - INFO - Completed parallel execution for 30 queries across 1 protocols\n",
      "2025-08-07 23:40:47,768 - INFO - Added 30 new learnings\n"
     ]
    }
   ],
   "source": [
    "research_state =  agent._execute_parallel_search_node(research_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:40:47,788 - INFO - Evaluating learnings at depth 2\n",
      "2025-08-07 23:40:47,791 - INFO - Total learnings so far: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'continue'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_state = agent._evaluate_learnings_node(research_state)\n",
    "agent._should_continue_research(research_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:40:47,818 - INFO - Evaluating current learnings for additional search needs...\n",
      "2025-08-07 23:40:47,822 - INFO - Executing LLM chain with params: dict_keys(['initial_idea', 'format_instructions', 'topics'])\n",
      "2025-08-07 23:40:50,215 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:40:50,222 - INFO - LLM chain executed successfully.\n",
      "2025-08-07 23:40:50,223 - INFO - No additional queries needed - research is sufficient.\n",
      "2025-08-07 23:40:50,226 - INFO - Generated 0 queries for depth 2\n"
     ]
    }
   ],
   "source": [
    "research_state = agent._generate_queries_node(research_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:40:50,249 - INFO - No queries to execute\n",
      "2025-08-07 23:40:50,258 - INFO - Evaluating learnings at depth 3\n",
      "2025-08-07 23:40:50,259 - INFO - Total learnings so far: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_state = agent._execute_parallel_search_node(research_state)\n",
    "research_state = agent._evaluate_learnings_node(research_state)\n",
    "agent._should_continue_research(research_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:40:50,278 - INFO - Generating knowledge base from research learnings...\n",
      "2025-08-07 23:40:50,281 - INFO - Executing LLM chain with params: dict_keys(['initial_idea', 'research_findings'])\n",
      "2025-08-07 23:41:32,089 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:41:32,152 - INFO - LLM chain executed successfully.\n",
      "2025-08-07 23:41:32,158 - INFO - Knowledge base generation completed\n",
      "2025-08-07 23:41:32,159 - INFO - Knowledge base generation completed\n"
     ]
    }
   ],
   "source": [
    "research_state = agent._generate_knowledge_base_node(research_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inidividual Agent Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:46:10,701 - INFO - Executing LLM chain with params: dict_keys(['initial_idea', 'research_findings'])\n",
      "2025-08-07 23:47:18,413 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 23:47:18,524 - INFO - LLM chain executed successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Knowledge Base for Building an AI-Powered Mutual Fund Recommendation System for Indian MFs\\n\\n---\\n\\n## 1. Executive Summary\\n\\nThis document provides a comprehensive knowledge base for developing an AI-powered mutual fund recommendation system tailored to Indian investors. The platform will fetch data from public/third-party APIs, visualize fund performance, track investments, suggest fund switches, analyze performance, recommend optimal funds based on risk profiles, and provide allocation strategies. The goal is to leverage existing open-source libraries and APIs, minimize redundant development, and ensure regulatory compliance and robust security.\\n\\n---\\n\\n## 2. Existing Solutions & Features to Leverage\\n\\n### Notable Platforms in India\\n- **Risify AI, InvestorAi, 5nance, ET Money Genius, Jarvis Invest, Fisdom, Wright Research, SBI SmartAssist**: Offer AI-driven recommendations, portfolio insights, risk profiling, and automation.\\n- **Value Research Online, RupeeVest, Kuvera, myCAMS, MFCentral, INDmoney**: Provide data aggregation, fund comparison, portfolio tracking, and goal-based investing.\\n\\n### Features to Leverage\\n- **Personalized Recommendations**: AI-driven, based on user risk profile and goals.\\n- **Portfolio Tracking**: Real-time NAV updates, consolidated views.\\n- **Visualization Tools**: Interactive charts, performance analytics.\\n- **Goal-Based Investing**: Aligning investments with user objectives.\\n- **Switch Suggestions & Rebalancing**: Automated, data-driven recommendations.\\n- **Educational Content**: Tooltips, guides, and market updates.\\n\\n---\\n\\n## 3. Technical Architecture & Stack Recommendations\\n\\n### Backend\\n- **Language**: Python (for ML/data), Node.js (for API orchestration)\\n- **Frameworks**: Django/Flask (Python), FastAPI (for REST APIs)\\n- **Database**: PostgreSQL (relational), MongoDB (unstructured data)\\n- **Data Processing**: Pandas, NumPy, Apache Kafka (for real-time ingestion)\\n- **ML/AI**: Scikit-learn, TensorFlow, PyTorch, XGBoost, LSTM/GRU for time series\\n- **Containerization**: Docker, Kubernetes (for scaling)\\n\\n### Frontend\\n- **Framework**: React.js (preferred for dashboards), Angular (alternative)\\n- **UI Libraries**: Ant Design Pro, CoreUI, Tremor, React-admin, ZenUI\\n- **Visualization**: Plotly, Seaborn, Matplotlib, Cufflinks, Altair\\n\\n### Infrastructure\\n- **Cloud**: AWS, Azure, or GCP (for managed services, scaling, and compliance)\\n- **Security**: OAuth2/JWT for authentication, SSL/TLS for data in transit\\n\\n---\\n\\n## 4. Feature Analysis (Core MVP vs. Nice-to-Have)\\n\\n| Feature                                 | MVP | Nice-to-Have |\\n|------------------------------------------|-----|--------------|\\n| User registration & KYC onboarding      | ✔   |              |\\n| Risk profiling questionnaire            | ✔   |              |\\n| Fetching & displaying MF data           | ✔   |              |\\n| Personalized fund recommendations       | ✔   |              |\\n| Portfolio tracking & visualization      | ✔   |              |\\n| Switch/rebalance suggestions            | ✔   |              |\\n| Performance analytics (returns, risk)   | ✔   |              |\\n| Goal-based investing modules            |     | ✔            |\\n| Advanced analytics (tax, scenario sim)  |     | ✔            |\\n| Social/community features               |     | ✔            |\\n| Customizable dashboards                 |     | ✔            |\\n| Educational content                     |     | ✔            |\\n| Multi-currency support                  |     | ✔            |\\n| Biometric/MFA authentication            |     | ✔            |\\n| Integration with brokerage for execution|     | ✔            |\\n\\n---\\n\\n## 5. User Experience & Journey Insights\\n\\n### Typical User Journey\\n1. **Onboarding**: Registration, KYC (paperless, eKYC), risk profiling.\\n2. **Discovery**: Fund search, personalized recommendations, comparison tools.\\n3. **Investment Execution**: SIP/lump-sum setup, payment integration (UPI, net banking).\\n4. **Portfolio Management**: Dashboard with real-time tracking, performance analytics, alerts.\\n5. **Continuous Engagement**: Notifications, educational resources, customer support.\\n\\n### UX Insights from Existing Solutions\\n- **Simplicity**: Clean, intuitive dashboards (Groww, Kuvera).\\n- **Personalization**: Tailored recommendations (ET Money, Scripbox).\\n- **Visualization**: Interactive, easy-to-understand charts (Value Research, INDmoney).\\n- **Mobile-first**: Essential for Indian market, but web platform for advanced analytics.\\n\\n---\\n\\n## 6. Implementation Resources (Libraries, APIs, Code Examples)\\n\\n### Data APIs\\n- **mftool**: Python library for Indian MF data (NAVs, scheme info, historical data).\\n- **MFAPI.in**: Free API for historical/latest MF prices.\\n- **amfi-nav-api**: REST API for AMFI NAV data.\\n- **Finark, Tarrakki, Nuvama, Captnemo, LegalRaasta**: For advanced data and transactions (may require partnership).\\n\\n### Open Source Codebases\\n- **Mutual Fund Mantis**: NAV analytics, quantstats integration.\\n- **Mutual Fund Analytical Dashboard**: Streamlit-based, risk metrics, Monte Carlo, portfolio analysis.\\n- **MFAnalysis**: Real-time data, interactive charts, MFAPI.in integration.\\n- **Fund-Management**: Portfolio analysis, AI-driven insights.\\n\\n### Visualization Libraries\\n- **Plotly**: Interactive, web-based charts.\\n- **Matplotlib/Seaborn**: Static/animated statistical plots.\\n- **Cufflinks**: Pandas-Plotly bridge for quick financial charts.\\n- **Altair**: Declarative, concise syntax for complex visuals.\\n\\n### UI Libraries\\n- **Ant Design Pro, CoreUI, Tremor, React-admin, ZenUI**: Pre-built components for dashboards.\\n\\n### ML/AI Libraries\\n- **Scikit-learn, XGBoost, LightGBM**: For classification, regression, and ensemble models.\\n- **TensorFlow, PyTorch**: Deep learning (LSTM/GRU for time series).\\n- **Statsmodels**: Time series and econometric analysis.\\n\\n---\\n\\n## 7. Gap Analysis\\n\\n### Gaps in Existing Solutions\\n- **Open, API-driven personalization**: Most platforms are closed; few offer developer-friendly APIs.\\n- **Advanced AI/ML explainability**: Black-box recommendations are common; explainable AI is rare.\\n- **Seamless integration with all AMCs**: Fragmented data sources, limited cross-AMC execution.\\n- **Scenario simulation & portfolio optimization**: Limited in-depth simulation tools for users.\\n- **Regulatory transparency**: Few platforms clearly communicate compliance and data privacy.\\n\\n### What Needs to be Built\\n- **Unified data ingestion layer**: Aggregates from AMFI, MFAPI, and other APIs.\\n- **Explainable AI recommendation engine**: Transparent, user-understandable logic.\\n- **Dynamic risk profiling**: Regularly updated based on user behavior and market changes.\\n- **Portfolio optimization module**: HRP, ML-based, and scenario simulation.\\n- **User-centric, modular dashboard**: Customizable, with advanced analytics for power users.\\n- **Compliance and privacy layer**: Automated KYC, consent management, audit logs.\\n\\n---\\n\\n## 8. Development Recommendations\\n\\n### Architecture\\n- **Modular microservices**: Separate services for data ingestion, recommendation engine, user management, and UI.\\n- **API-first**: RESTful APIs for all core services, enabling future mobile/web expansion.\\n- **Event-driven**: Use Kafka/RabbitMQ for real-time updates and notifications.\\n\\n### Functional Modules\\n1. **User Management**: Registration, KYC, authentication (OAuth2/JWT, MFA).\\n2. **Risk Profiling**: Questionnaire, behavioral analysis, periodic updates.\\n3. **Data Aggregation**: Fetch from mftool, MFAPI, AMFI, and others; store in normalized DB.\\n4. **Recommendation Engine**: ML models (ensemble, LSTM, XGBoost), explainable outputs.\\n5. **Portfolio Tracking**: Real-time NAVs, performance metrics, allocation breakdown.\\n6. **Visualization**: Interactive dashboards (Plotly, React).\\n7. **Switch/Rebalance Module**: Suggest optimal switches, simulate impact.\\n8. **Notification System**: Alerts for market changes, rebalancing, goal tracking.\\n9. **Compliance & Audit**: KYC, AML, consent management, audit logs.\\n\\n### Data Models\\n- **User**: id, profile, KYC status, risk profile, goals.\\n- **Fund**: scheme_code, name, AMC, category, NAV history, risk metrics, expense ratio.\\n- **Portfolio**: user_id, holdings (fund_id, units, purchase date, cost), performance.\\n- **Recommendation**: user_id, recommended_funds, rationale, timestamp.\\n- **Transaction**: user_id, fund_id, type (buy/sell/switch), amount, date.\\n\\n### APIs\\n- **/register, /login, /kyc**: User onboarding.\\n- **/profile/risk**: Risk profiling.\\n- **/funds/search, /funds/{id}**: Fund discovery.\\n- **/portfolio**: Portfolio view and analytics.\\n- **/recommendations**: Personalized suggestions.\\n- **/switch**: Switch/rebalance suggestions.\\n- **/notifications**: Alerts and updates.\\n\\n---\\n\\n## 9. UI/UX Recommendations\\n\\n### General Principles\\n- **Clarity & Simplicity**: Clean, minimal dashboards with clear calls to action.\\n- **Personalization**: Dynamic content based on user profile and behavior.\\n- **Visualization**: Use interactive charts for performance, allocation, and risk.\\n- **Mobile-first, Responsive**: Prioritize mobile usability, but offer advanced features on web.\\n- **Onboarding**: Guided, with tooltips and educational content.\\n- **Accessibility**: High-contrast themes, screen reader support.\\n\\n### Dashboard Features\\n- **Portfolio Overview**: Value, returns, allocation pie chart, recent transactions.\\n- **Fund Explorer**: Search, filter, compare funds, view risk/return metrics.\\n- **Recommendation Panel**: Top picks, rationale, risk alignment.\\n- **Switch Suggestions**: Highlight underperformers, suggest alternatives.\\n- **Goal Tracker**: Visual progress towards financial goals.\\n- **Alerts/Notifications**: Market events, rebalancing, KYC reminders.\\n\\n### Inspiration\\n- **InvestIQ, Finodex, Runrate**: For clarity, goal-based planning, and gamification.\\n- **Groww, Kuvera, ET Money**: For Indian market UX best practices.\\n\\n---\\n\\n## 10. Security, Compliance & Privacy\\n\\n- **SEBI, RBI, DPDP Act 2023**: Adhere to Indian regulatory requirements.\\n- **KYC/AML**: Integrate with CKYC/eKYC providers, maintain audit trails.\\n- **Data Encryption**: AES-256 for data at rest, SSL/TLS for data in transit.\\n- **Authentication**: OAuth2/JWT, optional MFA/biometrics.\\n- **Consent Management**: Explicit user consent for data processing, easy withdrawal.\\n- **Incident Response**: Regular audits, vulnerability testing, disaster recovery plans.\\n\\n---\\n\\n## 11. Evaluation & Benchmarking\\n\\n- **Performance Metrics**: Alpha, Beta, Sharpe Ratio, Expense Ratio, R-squared.\\n- **Benchmarks**: Compare recommendations to Nifty 50, Sensex, or relevant indices.\\n- **User Feedback**: Collect and analyze user reviews for continuous improvement.\\n- **A/B Testing**: Test recommendation algorithms and UI changes for effectiveness.\\n\\n---\\n\\n## 12. Implementation Roadmap\\n\\n1. **MVP Phase**\\n   - User onboarding, KYC, risk profiling\\n   - Data ingestion (mftool/MFAPI)\\n   - Basic fund search, portfolio tracking, and visualization\\n   - Simple ML-based recommendations\\n   - Core dashboard (React + Ant Design Pro)\\n   - Security/compliance basics\\n\\n2. **Expansion Phase**\\n   - Advanced analytics (scenario simulation, tax)\\n   - Portfolio optimization (HRP, ML-based)\\n   - Switch/rebalance automation\\n   - Goal-based investing modules\\n   - Mobile app (React Native)\\n   - Enhanced security (MFA, biometric)\\n   - Community/social features\\n\\n3. **Scaling & Partnerships**\\n   - Integration with brokerage APIs for execution\\n   - Partnerships with AMCs for deeper data\\n   - Regulatory certifications (SEBI IA, etc.)\\n\\n---\\n\\n## 13. References & Further Reading\\n\\n- [mftool documentation](https://mftool.readthedocs.io/)\\n- [MFAPI.in](https://www.mfapi.in/)\\n- [SEBI Investment Adviser Regulations](https://www.sebi.gov.in/legal/regulations/jul-2013/sebi-investment-advisers-regulations-2013-last-amended-on-january-03-2024-_34603.html)\\n- [DPDP Act 2023](https://www.meity.gov.in/digital-personal-data-protection-bill-2023)\\n- [Open-source dashboards](https://github.com/CaptMav09/Mutual-Fund-Analytical-Dashboard)\\n- [Portfolio optimization algorithms](https://en.wikipedia.org/wiki/Hierarchical_Risk_Parity)\\n- [UI inspiration](https://www.orbix.studio/works/investiq)\\n\\n---\\n\\n## 14. Actionable Next Steps\\n\\n1. **Finalize MVP feature set and architecture.**\\n2. **Set up data ingestion using mftool/MFAPI.**\\n3. **Develop user onboarding, KYC, and risk profiling modules.**\\n4. **Implement basic recommendation engine (Scikit-learn/XGBoost).**\\n5. **Build React-based dashboard using Ant Design Pro.**\\n6. **Integrate security and compliance measures from day one.**\\n7. **Plan for iterative user testing and feedback collection.**\\n\\n---\\n\\n**This knowledge base should serve as a living document—update as you progress, learn from user feedback, and regulatory changes.**'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_texts = [learning.combined_content for learning in research_state.learnings]\n",
    "learning_text = \"\\n\\n========\\n\\n\".join(learning_texts)\n",
    "template = (\n",
    "            \"You are a consultant the User has hired to help them build a product. Based on the initial product idea and all the research findings from your research agents is provided below, \"\n",
    "            \"The client wants to build the product - your primary goal is to enable them with comprehensive knowledge base that will help them make informed decisions.\"\n",
    "            \"The client will use this knowledge base to make decisions about how to execute the product idea. Focus on execution and implementation details. Prevent reinventing the wheel wherever possible by leveraging existing open source libraries. \"\n",
    "            \"Give sufficient details of all the libraries your suggest. If there is external data involved - make sure to give sufficient details of the data sources, data models, api etc..\"\n",
    "            \"Here are some suggestions for the knowledge base (use this as a seed, you are not restricted by the following list and feel free to add or remove any section):\"\n",
    "            \"1. Executive Summary \"\n",
    "            \"2. Existing Solutions and Features we can leverage for user product idea\"\n",
    "            \"3. Technical Architecture & Stack Recommendations \"\n",
    "            \"4. Feature Analysis (Core MVP vs Nice-to-have) \"\n",
    "            \"5. User Experience & Journey Insights for the user product idea and those already covered by existing solutions \"\n",
    "            \"6. Implementation Resources (Libraries, APIs, Code Examples) \"\n",
    "            \"7. Gap Analysis - what additional features do we need to implement to make the user product idea successful\"\n",
    "            \"8. Development Recommendations for architecture, functional modules, data models, APIs  \"\n",
    "            \"9. UI/UX recommendations for the user product idea\"\n",
    "            \"\\n\"\n",
    "            \"Initial Idea: {initial_idea}\\n\"\n",
    "            \"Research Findings: \\n{research_findings}\\n\"\n",
    "            \"\\n\"\n",
    "            \"Provide a well-structured, actionable knowledge base that will help in product development decisions.\"\n",
    "        )\n",
    "\n",
    "params = {\n",
    "    \"initial_idea\": initial_idea,\n",
    "    \"research_findings\": learning_text,\n",
    "}\n",
    "\n",
    "knowledge_base = agent._run_llm_step(\n",
    "    dict(), template, params,\n",
    "    \"Could not generate knowledge base from research findings.\",\n",
    "    model_override=\"gpt-4.1\",\n",
    "    temperature_override=0.\n",
    ")\n",
    "knowledge_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Generation of the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from io import StringIO\n",
    "from pprint import pprint\n",
    "\n",
    "class TechStack(BaseModel):\n",
    "    Layer: str\n",
    "    Option: str\n",
    "    WhyFitsMVP: str\n",
    "\n",
    "class DataSourceAPI(BaseModel):\n",
    "    Name: str\n",
    "    Documentation_Link: str\n",
    "    GettingStarted: str\n",
    "    LicenceCost: str\n",
    "    MCPIfAvailable: str\n",
    "\n",
    "class ThirdPartyComponents(BaseModel):\n",
    "    Name: str\n",
    "    Link: str\n",
    "    Purpose: str\n",
    "    DocumentationLink: str\n",
    "    APIReferenceLink: str\n",
    "\n",
    "class ComparableProducts(BaseModel):\n",
    "    Name: str\n",
    "    Link: str\n",
    "    TakeAway: str\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    ProductIdeaSummary: str\n",
    "    CoreMVPFeatures: List[str]\n",
    "    NiceToHaveFeatures: List[str]\n",
    "    TechStackRecommendation: List[TechStack]\n",
    "    DataSourcesAPIsStorageAndManagement: List[DataSourceAPI]\n",
    "    ReusableComponents: List[ThirdPartyComponents]\n",
    "    InspirationalProducts: List[ComparableProducts]\n",
    "    UIUXKitsExamples: List[str]\n",
    "    EffortvsImpactMatrix: str\n",
    "    DeploymentPath: str\n",
    "    CostandTimelineEstimate: str\n",
    "    KeyUserStories: List[str]\n",
    "    RisksandMitigations: str\n",
    "    SourceLinks: str\n",
    "\n",
    "    def to_markdown(self):\n",
    "        \"\"\"Converts the knowledge base into a markdown formatted string.\"\"\"\n",
    "        markdown_str = f\"# Product Knowledge Base\\n\\n\"\n",
    "\n",
    "        # Product Idea Summary\n",
    "        markdown_str += f\"## Product Idea Summary\\n{self.ProductIdeaSummary}\\n\\n\"\n",
    "\n",
    "        # Core MVP Features\n",
    "        markdown_str += \"## Core MVP Features\\n\"\n",
    "        for feature in self.CoreMVPFeatures:\n",
    "            markdown_str += f\"- {feature}\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        # Nice To Have Features\n",
    "        markdown_str += \"## Nice To Have Features\\n\"\n",
    "        for feature in self.NiceToHaveFeatures:\n",
    "            markdown_str += f\"- {feature}\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "        \n",
    "        # Key User Stories\n",
    "        markdown_str += \"## Key User Stories\\n\"\n",
    "        for story in self.KeyUserStories:\n",
    "            markdown_str += f\"- {story}\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        # Tech Stack Recommendation\n",
    "        markdown_str += \"## Tech Stack Recommendation\\n\"\n",
    "        markdown_str += \"| Layer | Option | Why it Fits MVP |\\n\"\n",
    "        markdown_str += \"|---|---|---|\\n\"\n",
    "        for item in self.TechStackRecommendation:\n",
    "            markdown_str += f\"| {item.Layer} | {item.Option} | {item.WhyFitsMVP} |\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        # Data Sources, APIs, Storage and Management\n",
    "        markdown_str += \"## Data Sources, APIs, Storage and Management\\n\"\n",
    "        markdown_str += \"| Name | Documentation | Getting Started | License/Cost | MCP if Available |\\n\"\n",
    "        markdown_str += \"|---|---|---|---|---|\\n\"\n",
    "        for item in self.DataSourcesAPIsStorageAndManagement:\n",
    "            markdown_str += f\"| {item.Name} | [Link]({item.Documentation_Link}) | {item.GettingStarted} | {item.LicenceCost} | {item.MCPIfAvailable} |\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        # Reusable Components\n",
    "        markdown_str += \"## Reusable Components\\n\"\n",
    "        markdown_str += \"| Name | Link | Purpose | Documentation | API Reference |\\n\"\n",
    "        markdown_str += \"|---|---|---|---|---|\\n\"\n",
    "        for item in self.ReusableComponents:\n",
    "            markdown_str += f\"| {item.Name} | [Link]({item.Link}) | {item.Purpose} | [Doc]({item.DocumentationLink}) | [API]({item.APIReferenceLink}) |\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        # Inspirational Products\n",
    "        markdown_str += \"## Inspirational Products\\n\"\n",
    "        markdown_str += \"| Name | Link | Takeaway |\\n\"\n",
    "        markdown_str += \"|---|---|---|\\n\"\n",
    "        for item in self.InspirationalProducts:\n",
    "            markdown_str += f\"| {item.Name} | [Link]({item.Link}) | {item.TakeAway} |\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "        \n",
    "        # UI/UX Kits & Examples\n",
    "        markdown_str += \"## UI/UX Kits & Examples\\n\"\n",
    "        for item in self.UIUXKitsExamples:\n",
    "            markdown_str += f\"- {item}\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        # Effort vs. Impact Matrix\n",
    "        markdown_str += f\"## Effort vs. Impact Matrix\\n{self.EffortvsImpactMatrix}\\n\\n\"\n",
    "\n",
    "        # Deployment Path\n",
    "        markdown_str += f\"## Deployment Path\\n{self.DeploymentPath}\\n\\n\"\n",
    "\n",
    "        # Cost and Timeline Estimate\n",
    "        markdown_str += f\"## Cost and Timeline Estimate\\n{self.CostandTimelineEstimate}\\n\\n\"\n",
    "\n",
    "        # Risks and Mitigations\n",
    "        markdown_str += f\"## Risks and Mitigations\\n{self.RisksandMitigations}\\n\\n\"\n",
    "\n",
    "        # Source Links\n",
    "        markdown_str += f\"## Source Links\\n{self.SourceLinks}\\n\\n\"\n",
    "\n",
    "        return markdown_str\n",
    "\n",
    "\n",
    "class SectionScore(BaseModel):\n",
    "    Dimension: str\n",
    "    Score: int\n",
    "    Comment: str\n",
    "    \n",
    "class HighPriorityIssue(BaseModel):\n",
    "    Issue: str\n",
    "    Impact: str\n",
    "    Fix: str\n",
    "\n",
    "class ImprovementTask(BaseModel):\n",
    "    SectionToFix: str\n",
    "    ExactChange: str\n",
    "    Why:str\n",
    "\n",
    "class Critique(BaseModel):\n",
    "    OverallScore: int\n",
    "    ScoreBreakdown: List[SectionScore]\n",
    "    HighPriorityIssues: List[HighPriorityIssue]\n",
    "    ImprovementTasks: List[ImprovementTask]\n",
    "\n",
    "    def to_markdown(self):\n",
    "        \"\"\"Converts the critique into a markdown formatted string.\"\"\"\n",
    "        markdown_str = \"## Critique Analysis\\n\\n\"\n",
    "\n",
    "        markdown_str += \"### Strengths\\n\"\n",
    "        if self.strengths:\n",
    "            for item in self.strengths:\n",
    "                markdown_str += f\"- {item}\\n\"\n",
    "        else:\n",
    "            markdown_str += \"No specific strengths identified.\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        markdown_str += \"### Weaknesses\\n\"\n",
    "        if self.weaknesses:\n",
    "            for item in self.weaknesses:\n",
    "                markdown_str += f\"- {item}\\n\"\n",
    "        else:\n",
    "            markdown_str += \"No specific weaknesses identified.\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        markdown_str += \"### Suggestions for Improvement\\n\"\n",
    "        if self.suggestions:\n",
    "            for item in self.suggestions:\n",
    "                markdown_str += f\"- {item}\\n\"\n",
    "        else:\n",
    "            markdown_str += \"No specific suggestions provided.\\n\"\n",
    "        markdown_str += \"\\n\"\n",
    "\n",
    "        return markdown_str\n",
    "\n",
    "class ResearchState(BaseModel):\n",
    "    initial_idea: str = Field(description=\"Initial idea for the research\")\n",
    "    curr_knowledge_base: KnowledgeBase = Field(default=\"\", description=\"Knowledge base from the list of learnings\")\n",
    "    critique: Critique = Field(default=\"\", description=\"Critique of the knowledge base\")\n",
    "    knowledge_bases: List[KnowledgeBase] = Field(default_factory=list, description=\"List of knowledge bases\")\n",
    "    messages: List[Dict[str, Any]] = Field(default_factory=list, description=\"List of messages\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def knowledge_base_topics(state: ResearchState) -> ResearchState:\n",
    "    pass\n",
    "\n",
    "async def knowledge_base_task(state: ResearchState) -> ResearchState:\n",
    "\n",
    "    prompt = f\"\"\"### System (Agent Role)  \n",
    "    You are an **Execution-MVP Research Agent**.  \n",
    "    Your mission is the following, using the information in the \"INPUTS\" section below - \n",
    "    1). Turn a *user-supplied product idea* into a **ready-to-build MVP blueprint**, using **web search** and only reputable public sources. \n",
    "    2). Optimise for speed-to-prototype and maximum reuse of existing tech.\n",
    "    3). Ensure Comprehensiveness of the knowledge base - it should be able to cover all the aspects of the product idea.\n",
    "    4). Critically evaluate the relevance of each section to the provided product idea. If a section is not applicable, omit it and briefly state why (e.g., \"Not applicable because this is a hardware product\")\n",
    "    --------------------------------------------------------------------\n",
    "    1️⃣ DELIVERABLES • JSON with the following keys (use exact IDs):\n",
    "\n",
    "    1. ProductIdeaSummary · ≤ 75-word recap in user’s terminology  \n",
    "    2. CoreMVPFeatures · list of features that are essential for the MVP\n",
    "    3. NiceToHaveFeatures · bullet list  \n",
    "    4. TechStackRecommendation · list of dictionaries with keys: Layer | Options | WhyFitsMVP \n",
    "    5. DataSourcesAndDataAPIs · List of dictionaries with keys: Name | Documentation_links | Getting_started | MCP\n",
    "    6. ReusableComponentsAndServices  (OpenSource Only)\n",
    "      • Name\n",
    "      • Link\n",
    "      • Purpose\n",
    "      • Documentation Link\n",
    "      • API Reference Link\n",
    "    7. ComparableProducts · (≤ 5 ) List of dictionaries with keys: Name | Link | Take-away\n",
    "    8. UIUXKitsExamples · bullet list (links or screenshots + licence)  \n",
    "    9. EffortvsImpactMatrix · Markdown Table, 2 × 2 table (Quick Win, Big Bet, Fill-Ins, Icebox)  \n",
    "    10. DeploymentPath · hosting, DB, CI/CD; free-tier notes  \n",
    "    11. CostandTimelineEstimate · rough ranges for build, infra, third-party fees  \n",
    "    12. KeyUserStories · List of stories in the form “As a [user], I want [action], so that [benefit].”  \n",
    "    13. RisksandMitigations · technical / regulatory blockers + mitigations  \n",
    "    14. SourceLinks · numbered list; cite as [S1], [S2] inline above\n",
    "\n",
    "    --------------------------------------------------------------------\n",
    "    2️⃣ RESEARCH METHOD  \n",
    "\n",
    "    • **Query Design** – start broad, then specialised per deliverable.  \n",
    "    • **Source Quality Filter** – prefer  \n",
    "      – Official docs / GitHub / vendor blogs < 18 months old  \n",
    "      – Authoritative review sites (G2, Gartner, etc.)  \n",
    "      – Standards or academic sources for compliance issues  \n",
    "    • **De-duplication** – keep best source when findings overlap.  \n",
    "    • **Citation** – append [S#] after every factual statement or link.  \n",
    "    • **Critical Thinking** – flag thin evidence with “⚠ Evidence weak”.\n",
    "\n",
    "    --------------------------------------------------------------------\n",
    "    4️⃣ STOP CONDITIONS  \n",
    "\n",
    "    • Stop when all 13 headers are filled **or** 60 outbound links reached.  \n",
    "    • If a section is genuinely N/A, write: `N/A – no relevant info found [Sx]`.\n",
    "\n",
    "    --------------------------------------------------------------------\n",
    "    5️⃣ DEPTH-OF-INQUIRY RULES  \n",
    "\n",
    "    For *each execution dimension* (Tech Stack, Inspirational Services,\n",
    "    Reusable Components, Feature Set, UI/UX, User Stories) do:\n",
    "\n",
    "    1. **Enumerate** at least N candidates (see table below).  \n",
    "    2. For **every candidate**, provide  \n",
    "      – **What it is** (≤ 15 words)  \n",
    "      – **Why it matters for this MVP** (value prop)  \n",
    "      – **Quick-start (3 steps)** – actionable integration outline  \n",
    "      – **Effort / Impact** – integers 1-5 each  \n",
    "      – **Caveats / Limitations** – licence, region, scaling, etc.  \n",
    "    3. **Evidence quota** – cite ≥ 3 unique sources per sub-dimension or mark `[Gap]`.  \n",
    "    4. If quota unmet, list search queries tried and label sub-dimension “Open-Question”.\n",
    "\n",
    "    | Dimension            | Minimum N |\n",
    "    |----------------------|-----------|\n",
    "    | Tech Stack options   | 5 |\n",
    "    | Inspirational services| 4 |\n",
    "    | Reusable components  | 6 |\n",
    "    | Core features        | 5 |\n",
    "    | UI/UX kits           | 3 |\n",
    "    | User stories         | 8 |\n",
    "\n",
    "    *Micro-format example (Tech Stack option)*  \n",
    "    ```markdown\n",
    "    ### Frontend Option 1 – Next.js 14 [S12]  \n",
    "    - **Why:** React ecosystem, SEO-friendly SSR, fast scaffold.  \n",
    "    - **Quick-start:**  \n",
    "      1. `npx create-next-app@latest`  \n",
    "      2. Add Tailwind → `npm i -D tailwindcss`  \n",
    "      3. Deploy free to Vercel → `vc deploy`  \n",
    "    - **Effort / Impact:** 2 / 5  \n",
    "    - **Caveats:** File routing unfamiliar to Angular devs.  \n",
    "    - **Sources:** [S12] [S13] [S15]  \n",
    "\n",
    "    INPUTS: \n",
    "    ---------------------------\n",
    "    User Supplied Product Idea: {state.initial_idea} \n",
    "    \n",
    "    \"\"\"\n",
    "    client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = await client.responses.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=state.messages,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"web_search_preview\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        text_format=KnowledgeBase\n",
    "    )\n",
    "\n",
    "    content = response.output_parsed\n",
    "    state.curr_knowledge_base = content\n",
    "    state.messages.append({\"role\": \"assistant\", \"content\": content.model_dump_json()})\n",
    "    return state\n",
    "\n",
    "async def knowledge_base_critique(state:ResearchState) -> ResearchState:\n",
    "    critique_conv = \"\"\"\n",
    "      > **Role**  \n",
    "      > You are a **Critique Agent**. Your task is to **assess the quality** of the above knowledge-base and to give **concrete, actionable instructions** for raising its quality to a publish-ready standard.\n",
    "      > Each Key in the knowledge base is a dimension of the research that needs to be evaluated.\n",
    "      > Your focus should be on factual correctness, complenetess and actionability of the knowledgebase - don't worry about formatting or grammatical errors.\n",
    "      > It is not compulsory for you to suggest any improvements if you don't find any issues - you can return empty lists below\n",
    "      > Score each dimension **1 – 5** (5 = excellent) and justify in ≤ 30 words. \n",
    "      > Evaluate the following dimensions as they apply to building the product by an LLM code assistant - to prevent hallucination and achieve determinism.\n",
    "      > A good knowledge base will have sufficient information for an LLM based code assistant to build the product unsupervised with as little as possible human intervention. \n",
    "      > A good knowledge base will have emphasis on log term maintainability and extensibility. The architecture is modular but managable.\n",
    "      --------------------------------------------------------------------\n",
    "      3️⃣ OUTPUT FORMAT (JSON with the following keys (use exact IDs))\n",
    "\n",
    "    1. Overall Score: **NN/50** : NN = Sum of scores of each dimension\n",
    "    2. Score-Breakdown: List of Dictionaries with keys Dimension | Score | Comment\n",
    "    3. HighPriorityIssues: (≤ 5): List of Dictionaries with keys Issue | Impact | Fix\n",
    "    4. Improvement-Instructions: (Atomic Tasks): List of Dictionaries with keys SectionToFix | ExactChange | Why\n",
    "    \"\"\"\n",
    "    state.messages = state.messages + [\n",
    "        {\"role\": \"user\", \"content\": critique_conv}\n",
    "    ]\n",
    "    \n",
    "    client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = await client.responses.parse(\n",
    "        model=\"gpt-5\",\n",
    "        input=state.messages,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"web_search\"\n",
    "            }\n",
    "        ],\n",
    "        text_format = Critique\n",
    "    )\n",
    "\n",
    "    #content = response.output[0].content[0].text\n",
    "    content = response.output_parsed\n",
    "    state.critique = content\n",
    "    state.messages = state.messages + [\n",
    "        {\"role\": \"assistant\", \"content\": content.model_dump_json()}\n",
    "    ]\n",
    "    return state\n",
    "\n",
    "async def implement_changes_task(state: ResearchState) -> ResearchState:\n",
    "    state.messages = state.messages + [{'role':'system', 'content': state.critique.model_dump_json()}, \n",
    "    {'role':'user', 'content': \"Given the above improvement tasks and high priority issues, can you make the changes in the above knowledge base and return the updated knowledge base?\"}\n",
    "    ]\n",
    "    client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = await client.responses.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=state.messages,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"web_search_preview\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        text_format=KnowledgeBase\n",
    "    )\n",
    "\n",
    "    kb_mod = response.output_parsed\n",
    "    state.knowledge_bases.append(state.curr_knowledge_base)\n",
    "    state.curr_knowledge_base = kb_mod\n",
    "    return state\n",
    "\n",
    "def purge_messages(state: ResearchState) -> ResearchState:\n",
    "    messages = []\n",
    "    messages.append(state.messages[0])\n",
    "    state.messages.append({\"role\": \"assistant\", \"content\": state.curr_knowledge_base.model_dump_json()})\n",
    "    state.messages = messages\n",
    "    return state\n",
    "\n",
    "async def generate_knowledge_base(initial_idea:str, num_enrichment_loops:int=2) -> ResearchState:\n",
    "    MAX_ITER = num_enrichment_loops\n",
    "    idea = initial_idea\n",
    "\n",
    "    state = ResearchState(initial_idea = idea)\n",
    "\n",
    "\n",
    "    logging.info(f\"generating knowledge_base\")\n",
    "    # ----- Generate / Revise KB -----\n",
    "    state = await knowledge_base_task(state)\n",
    "    logging.info(f\"knowledge_base generated\")\n",
    "\n",
    "    for i in range(1, MAX_ITER + 1):\n",
    "        logging.info(f\"iteration {i}\")\n",
    "        \n",
    "        # ----- Critique -----\n",
    "        logging.info(f\"critiquing knowledge_base\")\n",
    "        state = await knowledge_base_critique(state)\n",
    "        logging.info(f\"knowledge_base critiqued\")\n",
    "\n",
    "        if len(state.critique.ImprovementTasks) > 0:\n",
    "            # ----- Make Changes -----\n",
    "            logging.info(\"Running task to modify the knowledge base\")\n",
    "            state = await implement_changes_task(state)\n",
    "            logging.info(\"knowledge base modified\")\n",
    "            state = purge_messages(state)\n",
    "        else:\n",
    "            logging.info(\"No changes needed\")\n",
    "            break\n",
    "\n",
    "        #modify messages\n",
    "    else:\n",
    "        print(\"⚠ Max iterations reached; manual review recommended.\")\n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 04:33:33,360 - INFO - generating knowledge_base\n",
      "2025-08-12 04:33:57,274 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 04:33:57,545 - INFO - knowledge_base generated\n",
      "2025-08-12 04:33:57,545 - INFO - iteration 1\n",
      "2025-08-12 04:33:57,547 - INFO - critiquing knowledge_base\n",
      "2025-08-12 04:34:56,971 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 04:34:56,993 - INFO - knowledge_base critiqued\n",
      "2025-08-12 04:34:56,995 - INFO - Running task to modify the knowledge base\n",
      "2025-08-12 04:35:25,458 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 04:35:25,707 - INFO - knowledge base modified\n",
      "2025-08-12 04:35:25,708 - INFO - iteration 2\n",
      "2025-08-12 04:35:25,710 - INFO - critiquing knowledge_base\n",
      "2025-08-12 04:36:17,308 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 04:36:17,345 - INFO - knowledge_base critiqued\n",
      "2025-08-12 04:36:17,347 - INFO - Running task to modify the knowledge base\n",
      "2025-08-12 04:36:49,136 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 04:36:49,371 - INFO - knowledge base modified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Max iterations reached; manual review recommended.\n"
     ]
    }
   ],
   "source": [
    "knowledge_base = await generate_knowledge_base(initial_idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stage 1: Web Search for Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 05:01:38,102 - INFO - Executing LLM chain with params: dict_keys(['product_idea', 'format_instructions', 'section_name', 'section_content'])\n",
      "2025-08-12 05:01:40,621 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 05:01:40,691 - INFO - LLM chain executed successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class Websearch(BaseModel):\n",
    "    search_queries: List[str]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Websearch)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "template = (\"\"\"Role: You are an expert Research Strategist and Senior Product Analyst.\n",
    "\n",
    "Objective: Your goal is to generate a list of precise, high-impact web search queries to validate, enrich, and expand upon a specific section of a product's Knowledge Base.\n",
    "\n",
    "Context:\n",
    "\n",
    "Product Idea: {product_idea}\n",
    "Section Name: {section_name}\n",
    "Current Section Content:\n",
    "{section_content}\n",
    "Your Task: Based on the provided context, generate a list of 3-5 web search queries. These queries should be strategically designed to find information that will significantly improve the quality, accuracy, and depth of this section.\n",
    "\n",
    "Guidelines for Generating Queries:\n",
    "\n",
    "Prioritize Impact: Focus on queries that will yield the most valuable information. Avoid trivial or overly broad searches.\n",
    "Validate Key Assumptions: Generate queries to verify critical data points, such as pricing, project maintenance status, or best practices (e.g., \"[library name]\" GitHub issues and last commit date).\n",
    "Seek Deeper Insights: Create queries to find tutorials, implementation guides, or expert opinions (e.g., tutorial for integrating \"[API name]\" with [Tech Stack component]).\n",
    "Explore Alternatives: Formulate queries to discover competing or alternative solutions (e.g., \"[component name]\" vs [competitor] performance comparison 2024).\n",
    "Be Specific and Concise: Use keywords from the content. Keep queries short and to the point.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "params = {\"product_idea\": knowledge_base.initial_idea,\n",
    "            \"format_instructions\": format_instructions,\n",
    "            \"section_name\": \"CoreMVPFeatures\", \n",
    "            \"section_content\": knowledge_base.curr_knowledge_base.CoreMVPFeatures}\n",
    "\n",
    "websearch = interviewer._run_llm_step(dict(), template, params, \"Could not generate the list of web search queries.\", output_parser=parser, model_override=\"gpt-4.1\", temparature_override=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best public APIs for real-time Indian mutual fund data 2024',\n",
       " 'implementing interactive mutual fund performance charts in React',\n",
       " 'mutual fund portfolio tracking open source libraries India',\n",
       " 'algorithm for optimal mutual fund allocation based on risk profile',\n",
       " 'mutual fund switch recommendation system case studies India']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websearch.search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 05:02:58,251 - INFO - Performing web search for: best public APIs for real-time Indian mutual fund data 2024\n",
      "c:\\Code\\anaconda3\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "2025-08-12 05:02:58,684 - INFO - response: https://www.bing.com/search?q=best+public+APIs+for+real-time+Indian+mutual+fund+data+2024&filters=ex1%3A%22ez5_19947_20312%22 200\n",
      "2025-08-12 05:02:58,755 - INFO - Web search successful.\n",
      "2025-08-12 05:02:58,755 - INFO - Performing web search for: implementing interactive mutual fund performance charts in React\n",
      "c:\\Code\\anaconda3\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "2025-08-12 05:02:58,982 - INFO - response: https://www.bing.com/search?q=implementing+interactive+mutual+fund+performance+charts+in+React&filters=ex1%3A%22ez5_19947_20312%22 200\n",
      "2025-08-12 05:02:59,099 - INFO - Web search successful.\n",
      "2025-08-12 05:02:59,101 - INFO - Performing web search for: mutual fund portfolio tracking open source libraries India\n",
      "c:\\Code\\anaconda3\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "2025-08-12 05:02:59,305 - INFO - response: https://www.bing.com/search?q=mutual+fund+portfolio+tracking+open+source+libraries+India&filters=ex1%3A%22ez5_19947_20312%22 200\n",
      "2025-08-12 05:02:59,471 - INFO - Web search successful.\n",
      "2025-08-12 05:02:59,471 - INFO - Performing web search for: algorithm for optimal mutual fund allocation based on risk profile\n",
      "c:\\Code\\anaconda3\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "2025-08-12 05:02:59,699 - INFO - response: https://www.bing.com/search?q=algorithm+for+optimal+mutual+fund+allocation+based+on+risk+profile&filters=ex1%3A%22ez5_19947_20312%22 200\n",
      "2025-08-12 05:02:59,760 - INFO - Web search successful.\n",
      "2025-08-12 05:02:59,760 - INFO - Performing web search for: mutual fund switch recommendation system case studies India\n",
      "c:\\Code\\anaconda3\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "2025-08-12 05:02:59,984 - INFO - response: https://www.bing.com/search?q=mutual+fund+switch+recommendation+system+case+studies+India&filters=ex1%3A%22ez5_19947_20312%22 200\n",
      "2025-08-12 05:03:00,030 - INFO - Web search successful.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Web Search Results ---\n",
      "search_queries=['best public APIs for real-time Indian mutual fund data 2024', 'implementing interactive mutual fund performance charts in React', 'mutual fund portfolio tracking open source libraries India', 'algorithm for optimal mutual fund allocation based on risk profile', 'mutual fund switch recommendation system case studies India'] search_results=['No good DuckDuckGo Search Result was found', 'Mar 6, 2015 · My customer wants me to change the packing method and I tell him that I will implement this change in/on/to … May 24, 2020 · Hello, I was just wondering which one of these two prepositions is better in this context. It\\'s quite hard to … Nov 6, 2009 · Our customer have three new feature requests. We documented these feature requets. In order to start the … Oct 23, 2016 · So , according to you\" implementing\" is wrong too. Don\\'t you? I have a doubt here. I have read that … Jan 30, 2021 · Hi there, Even though i know the definitions of these verbs, i still confuse them time to time. I would like to clarify if …', 'mutual造句： 1、This is essential for our mutual benefit. 这对我们的共同利益是必要的。 2、She believed the regard to be mutual. 她相信这种敬重是相互的。 3、We hope to promote mutual trade. 我们希望促进我们之间的贸易。 4、It will be to our mutual disadvantage. 那将对我们双方都不利。 Jan 21, 2020 · origin拟合非线性曲线的时候，自己输入定义的函数式y=a+blnx，得到的结果是拟合不收敛，但是如果把lnx算… M 21 和 M 12 分别为 N 1 对 N 2 和 N 2 对 N 1 的互感（mutual inductance）。 由物理学已知， M 21 = M 12 = M， 所以，以后都写作 M 。 在图 5-1（a）所示电流 i 1 、 i 2 的方向下，根据右手定则，可知 Φ 11 和 Φ 12 方向相同， Φ 22 和 Φ 21 方向相同，称为磁通相助。 Nov 30, 2015 · 互信息 (Mutual Information)多大才算大？ 如题，互信息 (Mutual Information)由于没有上界。 似乎没有好的方法比较绝对大小。 比如相关性系数，主观上可以选择0.9以上算高。 或者0… 显示全部 关注者 155 被浏览 公募与私募，mutual fund与hedge fund？ 请问，如果私募股权基金是private equity fund的话，那么public equity fund的中文是什么？ 我听到很多业内人士把公募基金… 显示全部 关注者 41', '使用Latex插入伪代码比较方便，在word中怎么插入呢？ 2023年5月补充： 关于MDPI期刊的质量问题，之前陆续更新了几次回答，相信已经说得非常清楚了。我相信能找到这个帖子的人都 … 希望本文不仅能告诉你什么是动态规划，也能给你一种如何分析、求解动态规划问题的思考方式。 0001b 动态规划介绍 运筹学中的 … May 9, 2022 · 1.复杂的模型先用DM砍成规整的，方方正正的那种 2.先粗划分，再插入——方法——细化 3.砍成好几块后，分开 … 知乎，中文互联网高质量的问答社区和创作者聚集的原创内容平台，于 2011 年 1 月正式上线，以「让人们更好的分享知识、经验和见 …', 'mutual造句： 1、This is essential for our mutual benefit. 这对我们的共同利益是必要的。 2、She believed the regard to be mutual. 她相信这种敬重是相互的。 3、We hope to promote mutual … Jan 21, 2020 · origin拟合非线性曲线的时候，自己输入定义的函数式y=a+blnx，得到的结果是拟合不收敛，但是如果把lnx算… M 21 和 M 12 分别为 N 1 对 N 2 和 N 2 对 N 1 的互感（mutual inductance）。 由物理学已知， M 21 = M 12 = M， 所以，以后都写作 M 。 在图 5-1（a）所示电流 i 1 、 i 2 的方向下，根据右手定则，可知 … Nov 30, 2015 · 互信息 (Mutual Information)多大才算大？ 如题，互信息 (Mutual Information)由于没有上界。 似乎没有好的方法比较绝对大小。 比如相关性系数，主观上可以选择0.9以上算高。 或者0…  … 公募与私募，mutual fund与hedge fund？ 请问，如果私募股权基金是private equity fund的话，那么public equity fund的中文是什么？ 我听到很多业内人士把公募基金… 显示全部 关注者 41']\n"
     ]
    }
   ],
   "source": [
    "# Create a search query and fetch context from the web\n",
    "class SearchResuts(BaseModel):\n",
    "    search_queries: List[str] = []\n",
    "    search_results: List[str] = []\n",
    "\n",
    "    def str(self):\n",
    "        return '\\n'.join([f\"Query: {qa[0]}\\n Search Results: {qa[1]}\" for qa in zip(self.search_queries, self.search_results)])\n",
    "\n",
    "search_results = SearchResuts()\n",
    "for query in websearch.search_queries:\n",
    "    search_result = interviewer.search_web_for_context(query)\n",
    "    search_results.search_queries.append(query)\n",
    "    search_results.search_results.append(search_result)\n",
    "\n",
    "print(\"--- Web Search Results ---\")\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 02:21:21,458 - INFO - Performing web search for: latest AI-powered personal finance apps 2024\n",
      "c:\\Code\\anaconda3\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "2025-07-31 02:21:21,723 - INFO - response: https://www.bing.com/search?q=latest+AI-powered+personal+finance+apps+2024&filters=ex1%3A%22ez5_19934_20299%22 200\n",
      "2025-07-31 02:21:23,258 - INFO - response: https://www.bing.com/search?q=latest+AI-powered+personal+finance+apps+2024&filters=ex1%3A%22ez5_19934_20299%22&first=11&FORM=PERE 200\n",
      "2025-07-31 02:21:23,370 - INFO - Web search successful.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "interviewer.search_tool = DuckDuckGoSearchResults()\n",
    "query = websearch.search_queries[0]\n",
    "list_results = interviewer.search_tool.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snippet: Nov 6, 2024 · Here are some of the most popular, freely available, AI-powered budgeting and financial management tools used in India. The table below compares these tools based on …, title: Top 5 AI Budgeting Tools - Analytics Vidhya, link: https://www.analyticsvidhya.com/blog/2024/11/ai-budgeting-tools/, snippet: Jun 29, 2025 · AI finance tools are mobile apps or platforms that use artificial intelligence to track, analyze, and manage your personal finances. These tools study your spending habits, give …, title: Top AI Tools for Personal Finance in India: Your Smart Money …, link: https://taxreaders.com/top-ai-tools-for-personal-finance-in-india-your-smart-money-manager-in-2025-ai-tools-for-personal-finance-in-india/, snippet: Sep 21, 2024 · Managing personal finances can often feel overwhelming. Thankfully, AI-powered tools have emerged to simplify budgeting, saving, and managing expenses., title: 10 OF BEST & COMMONLY USED AI TOOLS FOR PERSONAL …, link: https://aienabledhub.com/blog/commonly-used-ai-tools-for-personal-finance/, snippet: Nov 8, 2024 · Explore the transformation of the best AI-powered apps for personal finance and budgeting. From tracking expenses to investment insights, these tools make achieving …, title: Top AI-Powered Apps for Personal Finance & Budgeting | 2024 Guide, link: https://shulikatata.com/ai-powered-apps-for-personal-finance-and-budgeting/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stage 2: Generate Initial Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first question and generate an initial answer based on the search context\n",
    "question = interviewer.get_next_question()\n",
    "initial_answer = interviewer.generate_initial_answer(mock_state, question, search_results)\n",
    "\n",
    "print(f\"--- Initial Answer for: '{question}' ---\")\n",
    "print(initial_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stage 3: Critique Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critique the generated answer for weaknesses or omissions\n",
    "critique = interviewer.critique_answer(mock_state, question, initial_answer)\n",
    "\n",
    "print(\"--- Critique of the Answer ---\")\n",
    "print(critique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stage 4: Refine Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine the initial answer based on the critique\n",
    "refined_answer = interviewer.refine_answer(mock_state, question, initial_answer, critique)\n",
    "\n",
    "print(\"--- Refined Answer ---\")\n",
    "print(refined_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Stage 5: Format for User Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the refined answer into a user-friendly presentation\n",
    "user_presentation = interviewer.present_to_user(mock_state, question, refined_answer)\n",
    "\n",
    "print(\"--- Formatted for User ---\")\n",
    "print(user_presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Full Self-Critique Workflow (All Stages Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the entire self-critique process for a single question\n",
    "print(f\"Running full self-critique for question: '{question}'\")\n",
    "full_result = interviewer.process_question_with_self_critique(mock_state, question)\n",
    "\n",
    "if 'error' in full_result:\n",
    "    print(f\"An error occurred: {full_result['error']}\")\n",
    "else:\n",
    "    print(\"--- Final User Presentation from Full Workflow ---\")\n",
    "    print(full_result.get('user_presentation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Full Interview Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate running the agent through the first 3 questions of the interview\n",
    "agent_for_run = SafeProductInterviewerAgent()\n",
    "current_state = {\"initial_idea\": initial_idea, \"qna_history\": {}}\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'--- Running Interview: Question {i+1} ---')\n",
    "    current_state = agent_for_run.run(current_state)\n",
    "    print(f'Q&A history now contains {len(current_state[\"qna_history\"])} items.')\n",
    "    print('--- End of Question ---\n",
    "')\n",
    "\n",
    "print(\"Full interview simulation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
